{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Contextual-Bandit-Network_RL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moustafa-7/Contextual-Bandint_RL_Agent/blob/master/Contextual_Bandit_Network_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z69Ve0GEFrlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpxoFldgF5Ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class contextual_bandit():\n",
        "  def __init__(self):\n",
        "    self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5]])\n",
        "    self.num_bandits = self.bandits.shape[0]\n",
        "    self.num_actions = self.bandits.shape[1]\n",
        "    \n",
        "    \n",
        "  def get_bandit(self):\n",
        "    self.state = np.random.randint(0,len(self.bandits))\n",
        "    return self.state\n",
        "  \n",
        "  def pull_arm(self,action):\n",
        "    result = np.random.randn(1)\n",
        "    bandit = self.bandits[self.state, action]\n",
        "    \n",
        "    if result > bandit:\n",
        "      return 1\n",
        "    \n",
        "    else:\n",
        "      return -1    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufk8s54lIYgx",
        "colab_type": "text"
      },
      "source": [
        "## Now build our Policy-Based Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enwo9Yo-HpM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class agent():\n",
        "  def __init__(self, lr, s_size, a_size):\n",
        "    self.state_in = tf.placeholder(shape =[1], dtype = tf.int32)\n",
        "    self.state_in_oh = slim.one_hot_encoding(self.state_in, s_size)\n",
        "    \n",
        "    output = slim.fully_connected(self.state_in_oh, a_size, activation_fn = tf.nn.sigmoid,\n",
        "                                 weights_initializer = tf.ones_initializer(),\n",
        "                                 biases_initializer = None)\n",
        "    \n",
        "    self.output = tf.reshape(output,[-1])\n",
        "    self.chosen_action = tf.argmax(self.output, 0)\n",
        "    \n",
        "    \n",
        "    self.reward_holder = tf.placeholder(shape = [1], dtype = tf.float32)\n",
        "    self.action_holder = tf.placeholder(shape = [1], dtype = tf.int32)\n",
        "    \n",
        "    self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
        "    self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
        "    \n",
        "    self.update = optimizer.minimize(self.loss)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsIfXwhVMjVr",
        "colab_type": "text"
      },
      "source": [
        "## Training the Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhVpq1eSMolI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ab109410-7b2e-44c3-f408-8c491dc1f3da"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "c_bandit = contextual_bandit()\n",
        "my_agent = agent(0.001, s_size = c_bandit.num_bandits, a_size = c_bandit.num_actions)\n",
        "\n",
        "weights = tf.trainable_variables()[0]\n",
        "\n",
        "num_episodes = 10000\n",
        "\n",
        "y = 0.99\n",
        "e = 0.1\n",
        "\n",
        "total_reward = np.zeros([c_bandit.num_bandits, c_bandit.num_actions])\n",
        "\n",
        "\n",
        "init = tf.initialize_all_variables()\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for i in range(num_episodes):\n",
        "    s = c_bandit.get_bandit()\n",
        "    \n",
        "    if np.random.randn(1) < e:\n",
        "      a = np.random.randint(c_bandit.num_actions) \n",
        "      \n",
        "    else:\n",
        "      a = sess.run(my_agent.chosen_action, feed_dict = {my_agent.state_in:[s]})\n",
        "      \n",
        "      \n",
        "    reward = c_bandit.pull_arm(a)\n",
        "    \n",
        "    feed_dict = {my_agent.action_holder: [a], my_agent.reward_holder: [reward], my_agent.state_in: [s]}\n",
        "    _, ww = sess.run([my_agent.update, weights], feed_dict = feed_dict)\n",
        "    \n",
        "    \n",
        "    \n",
        "    total_reward[s, a] += reward\n",
        "    \n",
        "    if(i % 500 == 0):\n",
        "      print(\"Mean reward for each of the \" + str(c_bandit.num_bandits) + \" bandits: \" +\n",
        "           str(np.mean(total_reward, axis = 1)))\n",
        "\n",
        "      \n",
        "      \n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward for each of the 3 bandits: [0.   0.25 0.  ]\n",
            "Mean reward for each of the 3 bandits: [23.   21.5  12.25]\n",
            "Mean reward for each of the 3 bandits: [45.   42.75 25.5 ]\n",
            "Mean reward for each of the 3 bandits: [72.75 64.   30.5 ]\n",
            "Mean reward for each of the 3 bandits: [100.5   88.    45.25]\n",
            "Mean reward for each of the 3 bandits: [125.   108.    52.25]\n",
            "Mean reward for each of the 3 bandits: [146.   126.    61.75]\n",
            "Mean reward for each of the 3 bandits: [170.5  150.5   68.75]\n",
            "Mean reward for each of the 3 bandits: [199.75 170.    78.  ]\n",
            "Mean reward for each of the 3 bandits: [227.   193.    86.25]\n",
            "Mean reward for each of the 3 bandits: [251.   210.75  91.5 ]\n",
            "Mean reward for each of the 3 bandits: [271.75 229.25 103.25]\n",
            "Mean reward for each of the 3 bandits: [292.25 245.75 111.25]\n",
            "Mean reward for each of the 3 bandits: [319.75 266.75 116.25]\n",
            "Mean reward for each of the 3 bandits: [345.5  285.   121.25]\n",
            "Mean reward for each of the 3 bandits: [372.25 301.5  130.  ]\n",
            "Mean reward for each of the 3 bandits: [396.75 319.   145.5 ]\n",
            "Mean reward for each of the 3 bandits: [419.25 339.25 150.25]\n",
            "Mean reward for each of the 3 bandits: [447.5  356.25 155.5 ]\n",
            "Mean reward for each of the 3 bandits: [474.5  377.5  163.75]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G97dTC5OUD9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "acf66c43-c474-43b8-be05-9a98d23a6193"
      },
      "source": [
        "\n",
        "for a in range(c_bandit.num_bandits):\n",
        "  print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \"for bandit \"+ str(a+1) +\n",
        "      \"is the most rewarding\")\n",
        "  if np.argmax(ww[a]) == np.argmin(c_bandit.bandits[a]):\n",
        "        print(\"...and it was right!\")\n",
        "  else:\n",
        "        print (\"...and it was wrong!\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The agent thinks action 4for bandit 1is the most rewarding\n",
            "...and it was right!\n",
            "The agent thinks action 2for bandit 2is the most rewarding\n",
            "...and it was right!\n",
            "The agent thinks action 1for bandit 3is the most rewarding\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}